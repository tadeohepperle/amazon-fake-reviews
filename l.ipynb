{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tadeo\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import typing\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer, BertTokenizer, AutoConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _ = AutoConfig.from_pretrained('bert-base-uncased', output_attention=True, return_unused_kwargs=True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello I am a robot</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun shines</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gurl.com was a US website for teenage girls th...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that Taingda Mingyi U Pho engineered the mass...</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>this is so cool</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   id\n",
       "0                                 Hello I am a robot    3\n",
       "1                                     The sun shines    4\n",
       "2  Gurl.com was a US website for teenage girls th...  234\n",
       "3   that Taingda Mingyi U Pho engineered the mass...  678\n",
       "4                                                hey    2\n",
       "5                                    this is so cool   98"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"text\": [\"Hello I am a robot\", \"The sun shines\", \"\"\"Gurl.com was a US website for teenage girls that was online from 1996 to 2018. It was created by Rebecca Odes, Esther Drill, and Heather McDonald as a resource centered on teen advice, body image, sexuality, and other teen concerns. First published as an online zine, it expanded into an online community. It was purchased in turn by Delia's, iVillage, PriMedia, and what became Defy Media. It ceased activity after Defy Media's closure in 2018 and was redirected to Seventeen's website. In the US, Gurl.com  \"\"\", \"\"\" that Taingda Mingyi U Pho engineered the massacre of around 40 members of the Burmese royal family in order to eliminate nearly all possible heirs to the throne?\n",
    "... that Tennessee State Route 396 was constructed to provide access to the Spring Hill Manufacturing plant of Saturn Corporation?\n",
    "... that when Arthur Forrest suggested that the squadron of French ships were looking for\"\"\", \"hey\", \"this is so cool\"], \"id\": [3,4,234,678,2,98]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello I am a robot',\n",
       " 'The sun shines',\n",
       " \"Gurl.com was a US website for teenage girls that was online from 1996 to 2018. It was created by Rebecca Odes, Esther Drill, and Heather McDonald as a resource centered on teen advice, body image, sexuality, and other teen concerns. First published as an online zine, it expanded into an online community. It was purchased in turn by Delia's, iVillage, PriMedia, and what became Defy Media. It ceased activity after Defy Media's closure in 2018 and was redirected to Seventeen's website. In the US, Gurl.com  \",\n",
       " ' that Taingda Mingyi U Pho engineered the massacre of around 40 members of the Burmese royal family in order to eliminate nearly all possible heirs to the throne?\\nthat Tennessee State Route 396 was constructed to provide access to the Spring Hill Manufacturing plant of Saturn Corporation?\\nthat when Arthur Forrest suggested that the squadron of French ships were looking for',\n",
       " 'hey',\n",
       " 'this is so cool']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = df[\"text\"].tolist()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7592, 1045, 2572, 1037, 8957, 102], [101, 1996, 3103, 12342, 2015, 102], [101, 19739, 12190, 1012, 4012, 2001, 1037, 2149, 4037, 2005, 9454, 3057, 2008, 2001, 3784, 2013, 2727, 2000, 2760, 1012, 2009, 2001, 2580, 2011, 9423, 24040, 2015, 1010, 14631, 12913, 1010, 1998, 9533, 9383, 2004, 1037, 7692, 8857, 2006, 9458, 6040, 1010, 2303, 3746, 1010, 13798, 1010, 1998, 2060, 9458, 5936, 1012, 2034, 2405, 2004, 2019, 3784, 1062, 3170, 1010, 2009, 4423, 2046, 2019, 3784, 2451, 1012, 2009, 2001, 4156, 1999, 2735, 2011, 3972, 2401, 1005, 1055, 1010, 4921, 9386, 3351, 1010, 3539, 9032, 1010, 1998, 2054, 2150, 13366, 2100, 2865, 1012, 2009, 7024, 4023, 2044, 13366, 2100, 2865, 1005, 1055, 8503, 1999, 2760, 1998, 2001, 2417, 7442, 10985, 2000, 9171, 1005, 1055, 4037, 1012, 1999, 1996, 2149, 1010, 19739, 12190, 1012, 4012, 102], [101, 2008, 13843, 3070, 2850, 11861, 10139, 1057, 6887, 2080, 13685, 1996, 9288, 1997, 2105, 2871, 2372, 1997, 1996, 14468, 2548, 2155, 1999, 2344, 2000, 11027, 3053, 2035, 2825, 15891, 2000, 1996, 6106, 1029, 2008, 5298, 2110, 2799, 4464, 2575, 2001, 3833, 2000, 3073, 3229, 2000, 1996, 3500, 2940, 5814, 3269, 1997, 14784, 3840, 1029, 2008, 2043, 4300, 16319, 4081, 2008, 1996, 3704, 1997, 2413, 3719, 2020, 2559, 2005, 102], [101, 4931, 102], [101, 2023, 2003, 2061, 4658, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = bert_tokenizer(ts)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tadeo\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7592, 1045, 2572, 1037, 8957, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 3103, 12342, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 19739, 12190, 1012, 4012, 2001, 1037, 2149, 4037, 2005, 9454, 3057, 2008, 2001, 3784, 2013, 2727, 2000, 2760, 1012, 2009, 2001, 2580, 2011, 9423, 24040, 2015, 1010, 14631, 12913, 1010, 1998, 9533, 9383, 2004, 1037, 7692, 8857, 2006, 9458, 6040, 1010, 2303, 3746, 1010, 13798, 1010, 1998, 2060, 9458, 5936, 1012, 2034, 2405, 2004, 2019, 3784, 1062, 3170, 1010, 2009, 4423, 2046, 2019, 3784, 2451, 1012, 2009, 2001, 4156, 1999, 2735, 2011, 3972, 2401, 1005, 1055, 1010, 4921, 9386, 3351, 1010, 3539, 9032, 1010, 1998, 2054, 2150, 13366, 2100, 2865, 1012, 2009, 7024, 4023, 2044, 13366, 2100, 2865, 1005, 1055, 8503, 1999, 2760, 1998, 2001, 2417, 7442, 10985, 2000, 9171, 1005, 1055, 4037, 1012, 1999, 1996, 2149, 1010, 19739, 12190, 1012, 4012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2008, 13843, 3070, 2850, 11861, 10139, 1057, 6887, 2080, 13685, 1996, 9288, 1997, 2105, 2871, 2372, 1997, 1996, 14468, 2548, 2155, 1999, 2344, 2000, 11027, 3053, 2035, 2825, 15891, 2000, 1996, 6106, 1029, 2008, 5298, 2110, 2799, 4464, 2575, 2001, 3833, 2000, 3073, 3229, 2000, 1996, 3500, 2940, 5814, 3269, 1997, 14784, 3840, 1029, 2008, 2043, 4300, 16319, 4081, 2008, 1996, 3704, 1997, 2413, 3719, 2020, 2559, 2005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4931, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2023, 2003, 2061, 4658, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = bert_tokenizer.batch_encode_plus(\n",
    "                ts,\n",
    "                max_length=200,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                return_token_type_ids=False,\n",
    "            )\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = {\n",
    "\n",
    "}\n",
    "\n",
    "k[\"input_ids\"] = torch.tensor(b[\"input_ids\"])[0:1,:]\n",
    "k[\"attention_mask\"] = torch.tensor(b[\"attention_mask\"])[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.6122, -0.5274,  0.3288,  ..., -0.2333,  0.3088, -0.1486],\n",
       "         [-0.9109,  0.7352, -0.3566,  ...,  0.0935,  0.0687, -0.2638],\n",
       "         [-1.2143,  0.7247,  0.0648,  ..., -1.0642,  1.2544,  0.6139],\n",
       "         ...,\n",
       "         [-1.5206,  0.5661,  1.0751,  ...,  1.0095, -0.3252, -0.3171],\n",
       "         [-1.8068, -0.0439,  0.2645,  ...,  0.5901, -0.2285,  0.2073],\n",
       "         [-1.1056,  0.4716,  1.6791,  ..., -0.2747, -1.2668,  0.1942]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.3591,  0.2789,  0.3123, -0.7421,  0.8098,  0.0654,  0.7938,  0.5096,\n",
       "          0.4401,  0.1535,  0.8207,  0.2657,  0.2544,  0.1746, -0.2899, -0.0951,\n",
       "         -0.8257, -0.4348, -0.4248, -0.3061,  0.3781,  0.1444,  0.7571, -0.4322,\n",
       "         -0.6205,  0.4381, -0.3023,  0.6006,  0.2809,  0.2075,  0.0861, -0.7003,\n",
       "          0.8956,  0.7011, -0.1795, -0.1698,  0.1590, -0.7048, -0.5642,  0.2017,\n",
       "         -0.4591, -0.1398, -0.1631,  0.2781,  0.1565, -0.0058,  0.2596, -0.5262,\n",
       "         -0.7661,  0.4931,  0.3588, -0.1759,  0.5040, -0.0102,  0.1414, -0.3993,\n",
       "          0.1242, -0.4268,  0.7128, -0.1979,  0.1023, -0.3933,  0.3027, -0.1992,\n",
       "          0.8399, -0.9348,  0.9166, -0.1221, -0.1958,  0.1127, -0.0821, -0.5115,\n",
       "          0.5518,  0.3121, -0.1355,  0.8553,  0.5991, -0.4175, -0.3402,  0.0083,\n",
       "          0.8871, -0.6475,  0.4445,  0.2302,  0.9251,  0.1392,  0.2836,  0.1352,\n",
       "         -0.1753,  0.5428, -0.3770,  0.8711,  0.2228,  0.6624,  0.3138,  0.2642,\n",
       "         -0.7336,  0.1432, -0.8942,  0.6587,  0.5642, -0.2632, -0.0448,  0.5234,\n",
       "         -0.3227, -0.7429,  0.4526,  0.8850,  0.2823, -0.5061,  0.0207, -0.1314,\n",
       "         -0.8388, -0.3420, -0.6436, -0.3234,  0.5866,  0.8724, -0.5234, -0.1423,\n",
       "         -0.2760, -0.0596, -0.5721, -0.3731, -0.0721, -0.4618,  0.0498,  0.0398,\n",
       "          0.4367,  0.2315, -0.6726,  0.5361,  0.0605,  0.0675, -0.3374, -0.1978,\n",
       "          0.2013, -0.1318,  0.2183,  0.5571, -0.6313, -0.2213,  0.4308, -0.8526,\n",
       "         -0.5440, -0.6919, -0.5870,  0.5482, -0.7555,  0.3358, -0.8157,  0.4016,\n",
       "         -0.0247,  0.7544,  0.1260, -0.0028, -0.7597, -0.6953,  0.3548, -0.5035,\n",
       "          0.0844, -0.6619, -0.8090,  0.0185,  0.0455, -0.6627, -0.2957, -0.7531,\n",
       "         -0.2999,  0.0853, -0.1406,  0.6826, -0.1079,  0.2845,  0.4076, -0.1573,\n",
       "         -0.5495, -0.1676, -0.4510, -0.0366,  0.2138,  0.1363, -0.5299,  0.6563,\n",
       "         -0.5140, -0.5320,  0.3498,  0.4333,  0.3021,  0.5706, -0.5512,  0.4530,\n",
       "         -0.3342, -0.0537,  0.5614,  0.1533,  0.1469, -0.5322, -0.5016,  0.3822,\n",
       "         -0.0797, -0.2496,  0.1822, -0.2164,  0.1663, -0.5249,  0.1464, -0.0262,\n",
       "          0.5977, -0.0417,  0.4489,  0.3381, -0.2052,  0.5940, -0.3732, -0.2332,\n",
       "         -0.1855, -0.7415,  0.2086, -0.1877,  0.0916,  0.3395, -0.2685,  0.2764,\n",
       "          0.6500,  0.3713, -0.2102,  0.6932,  0.1023,  0.2540,  0.2488,  0.6520,\n",
       "         -0.8001,  0.0536, -0.5437, -0.5757, -0.1140, -0.3903,  0.1040, -0.3072,\n",
       "         -0.7347, -0.4614, -0.3667, -0.2866,  0.4800,  0.1188,  0.7548, -0.0038,\n",
       "          0.5295,  0.6495,  0.7657,  0.4037, -0.1949, -0.0341,  0.6364,  0.0712,\n",
       "         -0.5267, -0.0207, -0.3364,  0.2294, -0.0943,  0.0862,  0.4358,  0.4629,\n",
       "         -0.0740,  0.3212,  0.4281, -0.1700, -0.1974, -0.3211,  0.3343,  0.1735,\n",
       "         -0.1296, -0.4325,  0.5364,  0.5534,  0.5538, -0.7775,  0.0096,  0.3003,\n",
       "         -0.7730,  0.0301, -0.4084, -0.2761, -0.0130, -0.1161,  0.2780, -0.3343,\n",
       "          0.4064,  0.2276,  0.4690,  0.6707, -0.5702, -0.1234,  0.5121,  0.1171,\n",
       "          0.4259, -0.5106,  0.1516, -0.5435,  0.3817, -0.6405,  0.4389, -0.5574,\n",
       "         -0.5906,  0.1768,  0.2308, -0.0694, -0.6726,  0.4618,  0.5412,  0.8371,\n",
       "          0.4352, -0.2541, -0.0367, -0.7949, -0.2146,  0.0430,  0.1346, -0.5304,\n",
       "          0.6016,  0.4804, -0.3806, -0.7726, -0.2824,  0.1402,  0.5516, -0.0836,\n",
       "         -0.2172,  0.6238, -0.7392,  0.6388,  0.6221, -0.3875, -0.4822, -0.6616,\n",
       "         -0.2601,  0.4113,  0.2691,  0.4727, -0.3328, -0.7703,  0.4431, -0.5816,\n",
       "          0.3168, -0.2945, -0.1825,  0.2801, -0.1741, -0.1537,  0.8021, -0.5978,\n",
       "         -0.3132, -0.2099,  0.4672, -0.0349,  0.5436,  0.5619, -0.3901,  0.3915,\n",
       "          0.6482, -0.6419,  0.9391,  0.4149,  0.2702, -0.0726, -0.2066, -0.0403,\n",
       "         -0.8021, -0.1543, -0.5089,  0.6225,  0.2808, -0.3421,  0.4257, -0.1564,\n",
       "          0.5581,  0.7759,  0.2984,  0.0956,  0.5867,  0.4391, -0.3554, -0.1541,\n",
       "         -0.3155, -0.3715,  0.1628,  0.1902, -0.2128,  0.2663, -0.5573, -0.7098,\n",
       "          0.1337,  0.0647,  0.1622,  0.3799, -0.0459,  0.3639, -0.1188,  0.6560,\n",
       "         -0.6515, -0.4318, -0.4810,  0.8062,  0.1593,  0.2425,  0.3652, -0.3944,\n",
       "         -0.0580,  0.2940,  0.9406, -0.0841,  0.8027,  0.0464, -0.0399, -0.7820,\n",
       "          0.5355,  0.1147, -0.0198, -0.1999,  0.2411,  0.1537, -0.7740, -0.0417,\n",
       "          0.1399, -0.5009,  0.4547,  0.6684, -0.0026, -0.0969,  0.2934,  0.2248,\n",
       "         -0.0323, -0.3936, -0.5609, -0.0465, -0.1299,  0.1889,  0.5663,  0.3973,\n",
       "          0.3734,  0.6452,  0.7585, -0.7512,  0.4371,  0.2978, -0.2183, -0.1086,\n",
       "         -0.1087, -0.8260,  0.2905,  0.6226,  0.5579, -0.6269,  0.7324, -0.4304,\n",
       "          0.1356,  0.6317,  0.0197,  0.4338, -0.0413,  0.0823, -0.2523, -0.5388,\n",
       "         -0.3823,  0.0148,  0.1478,  0.7180, -0.1971,  0.2024, -0.6547, -0.2486,\n",
       "         -0.3386, -0.1844, -0.6474,  0.0114,  0.1632,  0.4716,  0.0082, -0.3788,\n",
       "         -0.2616, -0.6206,  0.3948, -0.5458,  0.3760, -0.4877, -0.4086,  0.0370,\n",
       "          0.1113, -0.2011,  0.3216,  0.1965,  0.7336, -0.5072,  0.6006,  0.4753,\n",
       "          0.5837, -0.0823, -0.3597,  0.5120,  0.1106, -0.2539,  0.1219,  0.8532,\n",
       "          0.2112, -0.4911,  0.0391,  0.8829, -0.8458,  0.7081, -0.2330, -0.3845,\n",
       "          0.4090, -0.1640, -0.4342,  0.3003, -0.2962,  0.0806, -0.2635, -0.1645,\n",
       "          0.3861, -0.1634, -0.2336,  0.7840, -0.4543,  0.2652, -0.1857,  0.2598,\n",
       "          0.7688, -0.4182, -0.2682, -0.3836, -0.1100, -0.1627,  0.3380,  0.4143,\n",
       "         -0.4953,  0.3570,  0.3506,  0.4700, -0.3101, -0.2837, -0.3035, -0.6194,\n",
       "          0.3418,  0.0116, -0.8869, -0.3458,  0.3784,  0.0931, -0.1996,  0.6264,\n",
       "          0.1932, -0.2586,  0.5831,  0.5696, -0.1060, -0.2662,  0.2162, -0.6844,\n",
       "         -0.2247,  0.5719,  0.0166, -0.4221, -0.6447,  0.2985,  0.8940, -0.5362,\n",
       "          0.4345,  0.2440, -0.0876, -0.8786,  0.0483, -0.0342, -0.5891,  0.0070,\n",
       "         -0.0435,  0.5449,  0.3063, -0.7311,  0.5410,  0.2063,  0.4052, -0.3809,\n",
       "         -0.6177, -0.3779,  0.5772, -0.2098,  0.7838,  0.8096, -0.4585, -0.5014,\n",
       "          0.3468, -0.3060,  0.6473, -0.0021,  0.6095, -0.3714,  0.7980,  0.1182,\n",
       "         -0.1005,  0.0636, -0.7506,  0.1728,  0.1207, -0.8055, -0.4231,  0.0338,\n",
       "         -0.4100, -0.7178, -0.5079,  0.3667, -0.6321,  0.5317,  0.1027, -0.3675,\n",
       "         -0.4217, -0.0264, -0.2116, -0.1622, -0.2280, -0.4831, -0.2483,  0.0161,\n",
       "          0.7280, -0.0367, -0.4110,  0.2471,  0.8639,  0.7175, -0.4733, -0.1556,\n",
       "          0.3162, -0.0419, -0.0681, -0.0173, -0.1948, -0.6355,  0.5114, -0.0958,\n",
       "         -0.1469, -0.3093, -0.4021, -0.4432, -0.8651, -0.1340, -0.4127, -0.1069,\n",
       "         -0.2134,  0.0500, -0.1925,  0.3494, -0.2142, -0.0900, -0.2469, -0.1655,\n",
       "         -0.7667, -0.3743, -0.5712, -0.7528,  0.1278,  0.0216,  0.5318, -0.3379,\n",
       "          0.3842,  0.7131,  0.2316,  0.2329,  0.0989, -0.2023,  0.0320, -0.2698,\n",
       "          0.2711,  0.1986, -0.3280,  0.6220,  0.1903, -0.2524, -0.1971,  0.2729,\n",
       "          0.8077, -0.4149, -0.0597, -0.3696,  0.6533, -0.2711,  0.2706,  0.3062,\n",
       "         -0.7545,  0.2915,  0.1232, -0.5085, -0.3216,  0.3891, -0.0808, -0.4664,\n",
       "         -0.0840,  0.0540, -0.1731,  0.3145, -0.1187, -0.0933,  0.6080, -0.7126,\n",
       "         -0.8812, -0.0049,  0.0776, -0.2106,  0.5421, -0.4726, -0.4610, -0.4849,\n",
       "          0.7080,  0.1551,  0.3650,  0.4814,  0.1135, -0.6054,  0.3577,  0.5562,\n",
       "          0.1524, -0.3944,  0.0209,  0.1882,  0.9224, -0.0470, -0.1057, -0.6912,\n",
       "         -0.3746,  0.2125, -0.0949, -0.1553,  0.2664, -0.3399, -0.6098, -0.4493,\n",
       "          0.1024,  0.1529,  0.7006,  0.3541,  0.4632,  0.0491,  0.3212, -0.5473,\n",
       "          0.3478,  0.0239,  0.1390, -0.6282, -0.3599, -0.8756,  0.4311,  0.4350,\n",
       "         -0.2980, -0.4383,  0.7450,  0.6657, -0.2331, -0.1975,  0.0786, -0.4342,\n",
       "          0.1850,  0.5406,  0.0769,  0.4350,  0.0480, -0.1224, -0.1371, -0.0329]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = model(**k)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=6, step=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index[df[\"id\"] > 4].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, list([[1, 2, 3, 4, 5, 4, 7], [1, 1, 0, 0, 0, 0, 2]])],\n",
       "       [4, list([[1, 2, 3, 4, 5, 4, 7], [1, 1, 0, 0, 0, 0, 2]])],\n",
       "       [234, list([[1, 2, 3, 4, 5, 4, 7], [1, 1, 0, 0, 0, 0, 2]])],\n",
       "       [678, list([[1, 2, 3, 4, 5, 4, 7], [1, 1, 0, 0, 0, 0, 2]])],\n",
       "       [2, list([[1, 2, 3, 4, 5], [1, 1, 0, 0, 0]])],\n",
       "       [98, list([[1, 2, 3, 4, 5], [1, 1, 0, 0, 0]])]], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ten\"] = [ [[1,2,3,4,5], [1,1,0,0,0]] if e > 3 else [[1,2,3,4,5,4,7], [1,1,0,0,0,0,2]]  for e in range(0,6)]\n",
    "npdf = df[[\"id\", \"ten\"]].to_numpy()\n",
    "npdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\financial_technology\\amazon-fake-reviews\\l.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/code/financial_technology/amazon-fake-reviews/l.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mtensor(npdf)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "torch.tensor(npdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 1, 2, 5, 3, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zero_pad_ravel(tensor: torch.Tensor, X) -> torch.Tensor:\n",
    "    \"\"\"takes in a tensor of shape (N,D) appends zero elements or removes elements from the end to create an (X,D) shaped tensor and then reshapes that into a (X*D) shaped tensor\"\"\"\n",
    "    (N,D) = tensor.shape\n",
    "    return torch.reshape(torch.nn.functional.pad(input=tensor[0:X,:], pad=(0,0, 0, max(X-N, 0)), mode='constant', value=0), (-1,))\n",
    "            \n",
    "\n",
    "\n",
    "a = torch.tensor(np.array([[1,2,3],[1,2,5],[3,4,3]]))\n",
    "\n",
    "b = torch.tensor(np.array([[1],[5],[34]]))\n",
    "b.shape\n",
    "# torch.nn.functional.pad(input=a, pad=(0, -3, 4, 1), mode='constant', value=0)\n",
    "zero_pad_ravel(a, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "780e02f01b3ae00d5e97f14df45fcdece40e9a09f09f224add9952588133a4cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
